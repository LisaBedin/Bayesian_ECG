defaults:
  - _self_
  - experiment: ecg
  - algo: mgps  # _old

train: # Not used in generate.py
  name: null # Name of experiment (prefix of experiment name)
  ckpt_iter: max
  iters_per_ckpt: 10 # 1000
  iters_per_logging: 10000 # 100
  n_iters: 1000001
  learning_rate: 2e-4
  batch_size_per_gpu: 256 # 64 #  128  #  128  #  128  #  64  # 4
  # results_path: /lustre/fsn1/projects/rech/vpd/udw33dp/ecg_results/sashimi
  results_path: /mnt/data/lisa/ecg_results/sashimi

generate:
  ckpt_iter: max # Which checkpoint to use; assign a number or "max". Is ignored when sampling during training
  ckpt_smooth: null # Which checkpoint to start averaging from (experimental feature, can ignore)
  n_samples: 2 # Number of utterances to be generated (per GPU)
  batch_size: null # Number of samples to generate at once per GPU. null means max (equal to samples_per_gpu)

distributed:
  dist_backend: nccl
  dist_url: tcp://localhost:54321

denoising:
  # f_c_min: 0 #0
  # f_c_max: 1 # 0.7
  # l1reg: 10 # 0.1
  # l2reg: 10
  # J: 100
  # # n_leads: 9
  # amplitude: 1
  # # T: 20
  # # N: 50
  noise_type: 'bw' # 'em'
  # both_leads: True
  # #results_path: '/lustre/fsn1/projects/rech/vpd/udw33dp/ecg_results/denoising'
  # results_path: '/mnt/data/lisa/ecg_results/denoising'
  EM_step: 5
  alpha_lasso: 0.1
  alpha_ridge: 1

evaluate:
  #benchmark_folder: '/lustre/fsn1/projects/rech/vpd/udw33dp/ECG_inverse_problems_benchmark_data'
  # benchmark_folder: '/mnt/Reseau/Signal/lisa/ECG_inverse_problems_benchmark_data'
  train_downstream: 'gen'  #  'real'

wandb:
  mode: offline  # online # Pass in 'wandb.mode=online' to turn on wandb logging
  project: sashimi
  entity: phdlisa  # phdgabriel
  id: null # Set to string to resume logging from run
  job_type: training
